MGHD Repository Architecture - Training Dependencies
=====================================================

┌─────────────────────────────────────────────────────────────────────────────┐
│                           unified_mghd_optimizer.py                         │
│                          (Main Training Entry Point)                        │
├─────────────────────────────────────────────────────────────────────────────┤
│ • ComprehensiveMGHDOptimizer class                                         │
│ • run_foundation_train() - CUDA-Q training                                 │
│ • run_step11_garnet_train() - Legacy training                              │
│ • Optuna hyperparameter optimization                                       │
└─────────────────┬───────────────────────────────────────────────────────────┘
                  │
                  │ IMPORTS
                  ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                              Model Layer                                   │
├─────────────────┬───────────────────────────────────────────────────────────┤
│ poc_my_models.py│                     panq_functions.py                   │
├─────────────────┼───────────────────────────────────────────────────────────┤
│ • MGHD class    │                     • GNNDecoder class                  │
│ • ChannelSE     │                     • Training utilities                │
│ • FiLM          │                     • Data generation                    │
│                 │                     • Evaluation functions              │
└─────────────────┼───────────────────────────────────────────────────────────┘
                  │
                  │ DEPENDS ON
                  ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                           External Libraries                               │
├─────────────────┬─────────────────┬─────────────────┬─────────────────────┤
│    mamba_ssm    │     panqec      │     PyTorch     │     optuna          │
├─────────────────┼─────────────────┼─────────────────┼─────────────────────┤
│ • Mamba SSM     │ • Quantum codes  │ • NN framework │ • Hyper-opt         │
│ • Sequence      │ • Error models   │ • CUDA accel   │ • Trial mgmt        │
│   modeling      │ • Decoders       │ • DataLoader   │                     │
└─────────────────┴─────────────────┴─────────────────┴─────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                              Data Layer                                    │
├─────────────────┬───────────────────────────────────────────────────────────┤
│ tools/cudaq_    │                     tools/eval_ler.py                   │
│ sampler.py      │                                                         │
├─────────────────┼───────────────────────────────────────────────────────────┤
│ • CUDA-Q        │                     • LER evaluation                    │
│   integration   │                     • Wilson CIs                        │
│ • Fallback to   │                     • Latency benchmarks                │
│   numpy+LUT     │                     • Multiple decoders                 │
│ • Teacher       │                                                         │
│   labels        │                                                         │
└─────────────────┼───────────────────────────────────────────────────────────┘
                  │
                  │ USES
                  ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                           Specialized Components                           │
├─────────────────┬─────────────────┬─────────────────┬─────────────────────┤
│ cudaq_backend/  │   fastpath/     │   fastpath/     │   results/          │
│                 │                 │   c_api/        │                     │
├─────────────────┼─────────────────┼─────────────────┼─────────────────────┤
│ • Circuit noise │ • PyTorch CUDA  │ • C library     │ • Training outputs  │
│ • Syndrome gen  │   kernels       │ • Fast LUT      │ • Checkpoints       │
│ • Garnet model  │ • Persistent    │   decoding      │ • Metrics           │
│                 │   decoding      │                 │                     │
└─────────────────┴─────────────────┴─────────────────┴─────────────────────┘

Data Flow:
==========
1. unified_mghd_optimizer.py → poc_my_models.py (model definition)
2. unified_mghd_optimizer.py → panq_functions.py (training utilities)
3. unified_mghd_optimizer.py → tools/cudaq_sampler.py (data generation)
4. unified_mghd_optimizer.py → tools/eval_ler.py (evaluation)
5. poc_my_models.py → panq_functions.py (GNNDecoder base)
6. poc_my_models.py → mamba_ssm (Mamba SSM)
7. tools/cudaq_sampler.py → cudaq_backend/ (CUDA-Q integration)
8. tools/eval_ler.py → fastpath/ (fast decoding)

Key Dependencies:
================
• PyTorch: Core neural network framework
• panqec: Quantum error correction codes and models
• mamba_ssm: Mamba state space model implementation
• optuna: Hyperparameter optimization
• CUDA-Q: Quantum circuit simulation (with numpy fallback)
• MWPF/PyMatching: Classical decoder teachers
• fastpath: High-performance LUT decoding

Training Flow:
=============
1. Data generation (CUDA-Q or fallback)
2. Model training (MGHD or baseline GNN)
3. Hyperparameter optimization (Optuna)
4. Evaluation (LER with confidence intervals)
5. Checkpointing and artifact generation